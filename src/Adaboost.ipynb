## Overview of Boosting (AdaBoost)

Boosting is a machine learning technique that combines the predictions of multiple weak learners to create a strong learner. AdaBoost (Adaptive Boosting) is one of the most popular algorithms in this family. The main idea is to iteratively train weak classifiers on different distributions of the training data, focusing more on the data points that were previously misclassified.

### Advantages
- Can significantly improve the performance of weak learners.
- Reduces bias and variance, making it flexible and adaptive.
- Effective for both classification and regression tasks.

### Disadvantages
- Sensitive to noisy data and outliers.
- Computationally expensive due to the iterative nature of the training process.

---

## Representation

In AdaBoost, the final hypothesis $ H(x) $ is represented as a weighted sum of the weak learners $ h_t(x) $:

$$
H(x) = \text{sign} \left( \sum_{t=1}^{T} \alpha_t h_t(x) \right)
$$

Where:
-  $h_t(x) $ is the hypothesis from the weak learner at iteration $ t $.
- $ \alpha_t $ is the weight assigned to hypothesis $ h_t $, which is determined based on the classification accuracy.

---

## Loss Function

AdaBoost minimizes the exponential loss, which is given by:

$$
L(H) = \sum_{i=1}^{m} \exp(-y_i H(x_i))
$$

Where:
- $ y_i \in \{-1, +1\} $ is the true label for data point $ x_i $.
- $ H(x_i) $ is the combined prediction from all weak learners at point $ x_i $.

The exponential loss heavily penalizes misclassified examples, encouraging the model to focus on difficult cases.

---

## Optimizer

The optimizer in AdaBoost is not gradient-based but rather works by adjusting the distribution $ D $ of the training data based on the performance of the weak learner at each step. The update for the distribution after round $ t $ is:

$$
D_{t+1}(i) = \frac{D_t(i) \cdot \exp(-\alpha_t y_i h_t(x_i))}{Z_t}
$$

Where:
- $ D_t(i) $ is the weight of sample $ i $ at round $ t $.
- $ \alpha_t $ is the weight for weak learner $ h_t $.
- $ Z_t $ is a normalization factor to ensure that $ D_{t+1} $ is a valid probability distribution.

---

## Pseudocode

```python
# Pseudocode for AdaBoost

# Input: Training set S = {(x1, y1), (x2, y2), ..., (xm, ym)}, weak learner WL, number of rounds T
# Output: Final hypothesis H(x)

Initialize distribution D1(i) = 1/m for all i = 1 to m

for t = 1 to T:
    1. Train weak learner h_t using distribution Dt
    2. Compute error ε_t = sum(Dt(i) * [h_t(x_i) != y_i]) for all i
    3. Compute weight α_t = 0.5 * log((1 - ε_t) / ε_t)
    4. Update distribution:
        Dt+1(i) = Dt(i) * exp(-α_t * y_i * h_t(x_i))
        Normalize Dt+1 to make it a probability distribution
    
Output final hypothesis:
    H(x) = sign(sum(α_t * h_t(x) for t = 1 to T))
