{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part I: Overview of Boosting (AdaBoost)\n",
    "\n",
    "Boosting is a machine learning technique that combines the predictions of multiple weak learners to create a strong learner. AdaBoost (Adaptive Boosting) is one of the most popular algorithms in this family. The main idea is to iteratively train weak classifiers on different distributions of the training data, focusing more on the data points that were previously misclassified.\n",
    "\n",
    "### Advantages\n",
    "- Can significantly improve the performance of weak learners.\n",
    "- Reduces bias and variance, making it flexible and adaptive.\n",
    "- Effective for both classification and regression tasks.\n",
    "\n",
    "### Disadvantages\n",
    "- Sensitive to noisy data and outliers.\n",
    "- Computationally expensive due to the iterative nature of the training process.\n",
    "\n",
    "---\n",
    "\n",
    "## Representation\n",
    "\n",
    "In AdaBoost, the final hypothesis $ H(x) $ is represented as a weighted sum of the weak learners $ h_t(x) $:\n",
    "\n",
    "$$\n",
    "H(x) = \\text{sign} \\left( \\sum_{t=1}^{T} \\alpha_t h_t(x) \\right)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "-  $h_t(x) $ is the hypothesis from the weak learner at iteration $ t $.\n",
    "- $ \\alpha_t $ is the weight assigned to hypothesis $ h_t $, which is determined based on the classification accuracy.\n",
    "\n",
    "---\n",
    "\n",
    "## Loss Function\n",
    "\n",
    "AdaBoost minimizes the exponential loss, which is given by:\n",
    "\n",
    "$$\n",
    "L(H) = \\sum_{i=1}^{m} \\exp(-y_i H(x_i))\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $ y_i \\in \\{-1, +1\\} $ is the true label for data point $ x_i $.\n",
    "- $ H(x_i) $ is the combined prediction from all weak learners at point $ x_i $.\n",
    "\n",
    "The exponential loss heavily penalizes misclassified examples, encouraging the model to focus on difficult cases.\n",
    "\n",
    "---\n",
    "\n",
    "## Optimizer\n",
    "\n",
    "The optimizer in AdaBoost is not gradient-based but rather works by adjusting the distribution $ D $ of the training data based on the performance of the weak learner at each step. The update for the distribution after round $ t $ is:\n",
    "\n",
    "$$\n",
    "D_{t+1}(i) = \\frac{D_t(i) \\cdot \\exp(-\\alpha_t y_i h_t(x_i))}{Z_t}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $ D_t(i) $ is the weight of sample $ i $ at round $ t $.\n",
    "- $ \\alpha_t $ is the weight for weak learner $ h_t $.\n",
    "- $ Z_t $ is a normalization factor to ensure that $ D_{t+1} $ is a valid probability distribution.\n",
    "\n",
    "---\n",
    "\n",
    "## Pseudocode\n",
    "\n",
    "```python\n",
    "# Pseudocode for AdaBoost\n",
    "\n",
    "# Input: Training set S = {(x1, y1), (x2, y2), ..., (xm, ym)}, weak learner WL, number of rounds T\n",
    "# Output: Final hypothesis H(x)\n",
    "\n",
    "Initialize distribution D1(i) = 1/m for all i = 1 to m\n",
    "\n",
    "for t = 1 to T:\n",
    "    1. Train weak learner h_t using distribution Dt\n",
    "    2. Compute error ε_t = sum(Dt(i) * [h_t(x_i) != y_i]) for all i\n",
    "    3. Compute weight α_t = 0.5 * log((1 - ε_t) / ε_t)\n",
    "    4. Update distribution:\n",
    "        Dt+1(i) = Dt(i) * exp(-α_t * y_i * h_t(x_i))\n",
    "        Normalize Dt+1 to make it a probability distribution\n",
    "    \n",
    "Output final hypothesis:\n",
    "    H(x) = sign(sum(α_t * h_t(x) for t = 1 to T))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part II: Adaboost Stencil Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part III: Unit Tests\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part IV: Check Model\n",
    "### Breast Cancer Wisconsin Dataset\n",
    "\n",
    "The **Breast Cancer Wisconsin (Diagnostic) dataset** is commonly used to evaluate binary classification models due to its clean structure and medically relevant features. This dataset, obtained from the UCI Machine Learning Repository, contains 569 samples, each labeled as either **malignant** or **benign** based on various features computed from breast mass images.\n",
    "\n",
    "#### Objective\n",
    "Our goal is to apply the **Adaboost** algorithm to this dataset, leveraging decision stumps (i.e., single-depth decision trees) as weak learners. Adaboost iteratively adjusts the importance of misclassified samples, making it especially powerful for binary classification tasks with a strong focus on hard-to-classify instances.\n",
    "\n",
    "#### Methodology\n",
    "1. **Data Loading**: We load the data directly from a local file to avoid using pre-packaged datasets. The dataset includes 30 features that describe characteristics of the cell nuclei present in the image.\n",
    "2. **Preprocessing**: We convert the `Diagnosis` column to a binary format, where **1** represents malignant and **0** represents benign.\n",
    "3. **Training**: We apply Adaboost with 50 estimators (weak learners), where each estimator focuses on correcting the errors of the previous one.\n",
    "4. **Evaluation**: We evaluate the model on the test set using **Accuracy** and **F1 Score**, the latter of which provides a balanced measure between precision and recall, useful in medical diagnoses.\n",
    "\n",
    "#### Results\n",
    "The Adaboost model achieved:\n",
    "- **Accuracy**: 98.25%\n",
    "- **F1 Score**: 97.6%\n",
    "\n",
    "These results highlight Adaboost's robustness, achieving high performance on this diagnostic dataset, indicating its potential utility in medical classification tasks.\n",
    "\n",
    "#### Reference\n",
    "1. W.N. Street, W.H. Wolberg, and O.L. Mangasarian. \"Nuclear feature extraction for breast tumor diagnosis.\" *IS&T/SPIE International Symposium on Electronic Imaging: Science and Technology.* International Society for Optics and Photonics, 1993.\n",
    "2. Dua, D. and Graff, C. (2019). UCI Machine Learning Repository [https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+(Diagnostic)].\n",
    "\n",
    "#### Code\n",
    "The implementation can be found in the associated code cell, which trains the Adaboost model on the Breast Cancer Wisconsin dataset and evaluates its performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 98.25%\n",
      "F1 Score on test set: 97.60%\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "# Define file paths (update these paths if needed)\n",
    "data_path = '/Users/liuzhenke/Desktop/Brown CSCI/DATA2060 ML/Final Project/wdbc.data'\n",
    "\n",
    "# Define column names based on the dataset description\n",
    "column_names = ['ID', 'Diagnosis'] + [f'Feature_{i}' for i in range(1, 31)]\n",
    "\n",
    "# Load the data into a DataFrame\n",
    "data = pd.read_csv(data_path, header=None, names=column_names)\n",
    "\n",
    "# Drop ID column as it's not needed\n",
    "data = data.drop(columns=['ID'])\n",
    "\n",
    "# Convert Diagnosis column to binary values: M -> 1 (malignant), B -> 0 (benign)\n",
    "data['Diagnosis'] = data['Diagnosis'].map({'M': 1, 'B': 0})\n",
    "\n",
    "# Split features and target\n",
    "X = data.drop(columns=['Diagnosis'])\n",
    "y = data['Diagnosis']\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "\n",
    "# Initialize Adaboost with Decision Trees as weak learners\n",
    "adaboost_model = AdaBoostClassifier(n_estimators=50, random_state=42, algorithm='SAMME')\n",
    "\n",
    "# Train the model\n",
    "adaboost_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on test data\n",
    "y_pred = adaboost_model.predict(X_test)\n",
    "\n",
    "# Evaluate accuracy and F1 score\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "# Output results\n",
    "print(f\"Accuracy on test set: {accuracy:.2%}\")\n",
    "print(f\"F1 Score on test set: {f1:.2%}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part V: Contributions to the Project\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data2060",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
