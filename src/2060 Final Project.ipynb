{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part I: Overview of AdaBoost \n",
    "\n",
    "Boosting is an ensemble learning technique designed to improve model accuracy by combining weak learners, classifiers that perform only slightly better than random guessing, to create a powerful model. AdaBoost, short for Adaptive Boosting, was introduced by Freund and Schapire (1997) and has since become one of the most popular algorithms in this category. AdaBoost constructs a series of classifiers by iteratively focusing on samples that previous classifiers struggled to classify correctly (Freund and Schapire, 1997). This iterative approach enables the model to progressively improve accuracy by combining the knowledge of multiple classifiers.\n",
    "\n",
    "#### Algorithm Overview\n",
    "\n",
    "The main concept behind AdaBoost is that, given a base learner (often a decision stump), it can sequentially train weak learners on different distributions of the training data, emphasizing examples that were misclassified in previous rounds (Shalev-Shwartz and Ben-David, 2014). At each iteration, AdaBoost increases the weight of incorrectly classified examples, making the algorithm more \"sensitive\" to hard-to-classify cases. Once a new weak learner is trained, AdaBoost adjusts its weight based on its accuracy, giving more influence to learners that make fewer errors. The final model is a weighted sum of the individual classifiers, where each classifier's weight corresponds to its accuracy, forming a strong ensemble.\n",
    "\n",
    "---\n",
    "\n",
    "#### Representation\n",
    "\n",
    "In AdaBoost, the final hypothesis $ H(x) $ is represented as a weighted combination of the weak learners $ h_t(x) $:\n",
    "\n",
    "$$\n",
    "H(x) = \\text{sign} \\left( \\sum_{t=1}^{T} \\alpha_t h_t(x) \\right)\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $ h_t(x) $ is the prediction of the weak learner at iteration $ t $,\n",
    "- $ \\alpha_t $ is the weight assigned to hypothesis $ h_t $, calculated based on the classification accuracy at round $ t $. \n",
    "\n",
    "This weighted combination allows AdaBoost to leverage the accuracy of each weak learner, forming a strong final classifier that benefits from the collective performance of all weak learners (Freund and Schapire, 1997).\n",
    "\n",
    "---\n",
    "\n",
    "#### Loss Function\n",
    "\n",
    "AdaBoost minimizes an exponential loss function, which is given by:\n",
    "\n",
    "$$\n",
    "L(H) = \\sum_{i=1}^{m} \\exp(-y_i H(x_i))\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $ y_i \\in \\{-1, +1\\} $ is the true label for data point $ x_i $,\n",
    "- $ H(x_i) $ represents the combined prediction from all weak learners at $ x_i $.\n",
    "\n",
    "The exponential loss penalizes misclassified examples heavily, encouraging the model to focus on these challenging cases, which makes AdaBoost responsive to difficult examples in the data (Freund and Schapire, 1997).\n",
    "\n",
    "---\n",
    "\n",
    "#### Optimizer\n",
    "\n",
    "AdaBoost does not employ traditional gradient-based optimization but instead adjusts the distribution $ D $ of the training data based on the weak learner’s performance in each round. After each iteration, the distribution $ D $ is updated as follows:\n",
    "\n",
    "$$\n",
    "D_{t+1}(i) = \\frac{D_t(i) \\cdot \\exp(-\\alpha_t y_i h_t(x_i))}{Z_t}\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $ D_t(i) $ is the weight of sample $ i $ at round $ t $,\n",
    "- $ \\alpha_t = 0.5 \\cdot \\log\\left(\\frac{1 - \\epsilon_t}{\\epsilon_t}\\right) $ is the weight for the weak learner $ h_t $,\n",
    "- $ Z_t $ is a normalization factor ensuring that $ D_{t+1} $ remains a valid probability distribution.\n",
    "\n",
    "This reweighting process is central to AdaBoost’s adaptive capability, progressively focusing on difficult samples and dynamically adjusting to the data’s complexity (Shalev-Shwartz and Ben-David, 2014).\n",
    "\n",
    "---\n",
    "\n",
    "#### Advantages and Applications\n",
    "\n",
    "AdaBoost has several notable advantages:\n",
    "- **Improvement of Weak Learners**: Since it combines weak classifiers, AdaBoost significantly enhances the overall performance of each individual classifier, even if each performs only slightly better than random (Freund & Schapire, 1997).\n",
    "- **Reduction of Bias and Variance**: AdaBoost is known for its ability to reduce bias and variance, enhancing robustness across diverse datasets (Shalev-Shwartz and Ben-David, 2014).\n",
    "- **Versatility Across Domains**: AdaBoost adapts well across domains, and it is used for both classification and regression tasks. It has been applied to various fields, from face recognition and text classification to medical diagnosis, where small incremental improvements in prediction accuracy are highly valuable.\n",
    "\n",
    "In practical applications, AdaBoost’s adaptive nature makes it particularly effective in tasks with complex and high-dimensional data. For instance, Freund and Schapire applied AdaBoost to face detection, one of the early high-impact uses of the algorithm (Freund and Schapire, 1997). Its ability to adaptively combine classifiers with weighted predictions has made it popular in scenarios where the cost of errors is high, such as in medical diagnostics or fraud detection (Shalev-Shwartz and Ben-David, 2014).\n",
    "\n",
    "#### Disadvantages\n",
    "\n",
    "While AdaBoost is powerful, it has limitations:\n",
    "- **Sensitivity to Noise**: AdaBoost’s emphasis on misclassified samples can amplify the effect of noisy data. If a sample is mislabeled or contains outliers, AdaBoost may focus on these cases excessively, potentially destabilizing the model (Shalev-Shwartz and Ben-David, 2014).\n",
    "- **Computational Intensity**: The iterative training process requires substantial computational power, especially with large datasets or high-dimensional feature spaces. This makes AdaBoost resource-intensive, particularly when using complex base learners (Freund and Schapire, 1997).\n",
    "\n",
    "\n",
    "#### References\n",
    "Freund, Y. and Schapire, R.E., 1997. A Decision-Theoretic Generalization of On-Line Learning and an Application to Boosting. Journal of Computer and System Sciences, 55(1), pp.119-139.\n",
    "\n",
    "Shalev-Shwartz, S. and Ben-David, S., 2014. Understanding Machine Learning: From Theory to Algorithms. Cambridge University Press.\n",
    "\n",
    "\n",
    "#### Pseudocode\n",
    "\n",
    "```python\n",
    "# Pseudocode for AdaBoost\n",
    "\n",
    "# Input: Training set S = {(x1, y1), (x2, y2), ..., (xm, ym)}, weak learner WL, number of rounds T\n",
    "# Output: Final hypothesis H(x)\n",
    "\n",
    "Initialize distribution D1(i) = 1/m for all i = 1 to m\n",
    "\n",
    "for t = 1 to T:\n",
    "    1. Train weak learner h_t using distribution Dt\n",
    "    2. Calculate error ε_t = sum(Dt(i) * [h_t(x_i) != y_i]) for all i\n",
    "    3. Compute α_t = 0.5 * log((1 - ε_t) / ε_t)\n",
    "    4. Update distribution:\n",
    "        Dt+1(i) = Dt(i) * exp(-α_t * y_i * h_t(x_i))\n",
    "        Normalize Dt+1 to maintain a probability distribution\n",
    "    \n",
    "Output final hypothesis:\n",
    "    H(x) = sign(sum(α_t * h_t(x) for t = 1 to T))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part II: Adaboost Stencil Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "class DecisionStump:\n",
    "    def __init__(self):\n",
    "        # Initialize the decision stump with default values\n",
    "        self.polarity = 1            # Determines whether threshold comparison is < or >\n",
    "        self.threshold = None         # Threshold value for the feature\n",
    "        self.feature_idx = None       # Index of the feature this stump uses\n",
    "        self.alpha = None             # Weight of this stump in the final prediction\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Make predictions for data X using the decision stump's threshold and polarity.\n",
    "        Returns:\n",
    "            preds: array of predictions (1 or -1) for each sample in X\n",
    "        \"\"\"\n",
    "        n_samples = X.shape[0]\n",
    "        X_c = X[:, self.feature_idx]\n",
    "        preds = np.ones(n_samples)  # Initialize predictions to 1\n",
    "        # Apply polarity and threshold to determine predictions\n",
    "        if self.polarity == 1:\n",
    "            preds[X_c < self.threshold] = -1\n",
    "        else:\n",
    "            preds[X_c > self.threshold] = -1\n",
    "        return preds\n",
    "\n",
    "class myAdaBoost:\n",
    "    def __init__(self, n_clf=50):\n",
    "        \"\"\"\n",
    "        Initialize the AdaBoost classifier with a specified number of weak classifiers.\n",
    "        Args:\n",
    "            n_clf: The number of weak classifiers (decision stumps) to be created.\n",
    "        \"\"\"\n",
    "        self.n_clf = n_clf   # Number of classifiers\n",
    "        self.clfs = []       # List to store weak classifiers\n",
    "\n",
    "    def _initialize_weights(self, n_samples):\n",
    "        \"\"\"\n",
    "        Initialize sample weights to be equal for all samples.\n",
    "        Args:\n",
    "            n_samples: Number of samples in the dataset.\n",
    "        Returns:\n",
    "            w: Array of initialized weights for each sample.\n",
    "        \"\"\"\n",
    "        return np.full(n_samples, (1 / n_samples))\n",
    "\n",
    "    def _find_best_stump(self, X, y, w):\n",
    "        \"\"\"\n",
    "        Find the best decision stump (weak classifier) for the current sample weights.\n",
    "        Args:\n",
    "            X: Feature matrix.\n",
    "            y: True labels.\n",
    "            w: Sample weights.\n",
    "        Returns:\n",
    "            best_stump: A decision stump with optimized threshold, feature, and polarity.\n",
    "        \"\"\"\n",
    "        n_samples, n_features = X.shape\n",
    "        best_stump = DecisionStump()  # Initialize an empty decision stump\n",
    "        min_error = float('inf')      # Initialize minimum error as infinity\n",
    "\n",
    "        # Iterate over each feature to find the optimal threshold and polarity\n",
    "        for feat_idx in range(n_features):\n",
    "            thresholds = np.unique(X[:, feat_idx])  # Unique thresholds for current feature\n",
    "            for threshold in thresholds:\n",
    "                for polarity in [1, -1]:  # Test both polarity options\n",
    "                    error = self._calculate_stump_error(X, y, w, feat_idx, threshold, polarity)\n",
    "                    # Update best stump if current stump's error is lower\n",
    "                    if error < min_error:\n",
    "                        min_error = error\n",
    "                        best_stump.feature_idx = feat_idx\n",
    "                        best_stump.threshold = threshold\n",
    "                        best_stump.polarity = polarity\n",
    "\n",
    "        best_stump.alpha = self._calculate_alpha(min_error)  # Calculate the alpha for best stump\n",
    "        return best_stump\n",
    "\n",
    "    def _calculate_stump_error(self, X, y, w, feat_idx, threshold, polarity):\n",
    "        \"\"\"\n",
    "        Calculate the weighted classification error for a given stump configuration.\n",
    "        Args:\n",
    "            X: Feature matrix.\n",
    "            y: True labels.\n",
    "            w: Sample weights.\n",
    "            feat_idx: Feature index for the stump.\n",
    "            threshold: Threshold value for the stump.\n",
    "            polarity: Polarity of the stump (1 for '<' and -1 for '>').\n",
    "        Returns:\n",
    "            error: Weighted error of the stump.\n",
    "        \"\"\"\n",
    "        preds = np.ones(y.shape)      # Initialize all predictions to 1\n",
    "        X_feat = X[:, feat_idx]       # Values of current feature\n",
    "        if polarity == 1:\n",
    "            preds[X_feat < threshold] = -1\n",
    "        else:\n",
    "            preds[X_feat > threshold] = -1\n",
    "        error = np.sum(w[y != preds]) # Sum weights where prediction is incorrect\n",
    "        return error\n",
    "\n",
    "    def _calculate_alpha(self, error):\n",
    "        \"\"\"\n",
    "        Calculate the weight (alpha) for a weak classifier based on its error rate.\n",
    "        Args:\n",
    "            error: Error rate of the weak classifier.\n",
    "        Returns:\n",
    "            alpha: Weight of the weak classifier.\n",
    "        \"\"\"\n",
    "        EPS = 1e-10  # Small epsilon to prevent division by zero\n",
    "        return 0.5 * np.log((1 - error + EPS) / (error + EPS))\n",
    "\n",
    "    def _update_weights(self, w, alpha, y, preds):\n",
    "        \"\"\"\n",
    "        Update sample weights based on classifier performance and predictions.\n",
    "        Args:\n",
    "            w: Current sample weights.\n",
    "            alpha: Weight of the classifier.\n",
    "            y: True labels.\n",
    "            preds: Predictions made by the classifier.\n",
    "        Returns:\n",
    "            w: Updated sample weights.\n",
    "        \"\"\"\n",
    "        # Adjust weights: Increase weights for misclassified samples\n",
    "        w *= np.exp(-alpha * y * preds)\n",
    "        w /= np.sum(w)  # Normalize weights to maintain a distribution\n",
    "        return w\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Train the AdaBoost model by fitting multiple decision stumps on the data.\n",
    "        Args:\n",
    "            X: Feature matrix.\n",
    "            y: True labels.\n",
    "        \"\"\"\n",
    "        n_samples, _ = X.shape\n",
    "        w = self._initialize_weights(n_samples)  # Initialize sample weights\n",
    "        for _ in range(self.n_clf):\n",
    "            # Find the best decision stump given current weights\n",
    "            stump = self._find_best_stump(X, y, w)\n",
    "            preds = stump.predict(X)  # Get predictions from the stump\n",
    "\n",
    "            # Update weights based on the stump's performance\n",
    "            w = self._update_weights(w, stump.alpha, y, preds)\n",
    "            self.clfs.append(stump)   # Store the trained stump\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Make predictions for data X by combining predictions of all weak classifiers.\n",
    "        Args:\n",
    "            X: Feature matrix.\n",
    "        Returns:\n",
    "            y_pred: Final predictions for each sample.\n",
    "        \"\"\"\n",
    "        clf_preds = [clf.alpha * clf.predict(X) for clf in self.clfs]  # Weighted predictions\n",
    "        y_pred = np.sum(clf_preds, axis=0)  # Sum all weighted predictions\n",
    "        return np.sign(y_pred).astype(int)  # Convert sum to final label (1 or -1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part III: Check Model\n",
    "\n",
    "#### Unit Test\n",
    "\n",
    "We will use below unit test to ensure our methods and class work properorly and handles corner cases.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated weights (before normalization): [0.13447071 0.36552929 0.36552929 0.13447071]\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Test 11 Failed: Weights did not increase for misclassified samples",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 93\u001b[0m\n\u001b[0;32m     91\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m np\u001b[38;5;241m.\u001b[39misclose(np\u001b[38;5;241m.\u001b[39msum(updated_weights), \u001b[38;5;241m1\u001b[39m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTest 11 Failed: Weights do not sum to 1\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     92\u001b[0m \u001b[38;5;66;03m# Verify that misclassified weights increased\u001b[39;00m\n\u001b[1;32m---> 93\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m updated_weights[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m>\u001b[39m w_test[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;129;01mand\u001b[39;00m updated_weights[\u001b[38;5;241m2\u001b[39m] \u001b[38;5;241m>\u001b[39m w_test[\u001b[38;5;241m2\u001b[39m], \\\n\u001b[0;32m     94\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTest 11 Failed: Weights did not increase for misclassified samples\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     97\u001b[0m \u001b[38;5;66;03m# Test 12: Test _find_best_stump with trivial data\u001b[39;00m\n\u001b[0;32m     98\u001b[0m \u001b[38;5;66;03m# Check that the best stump is correctly identified on a linearly separable dataset\u001b[39;00m\n\u001b[0;32m     99\u001b[0m X_simple \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([[\u001b[38;5;241m0\u001b[39m], [\u001b[38;5;241m1\u001b[39m], [\u001b[38;5;241m2\u001b[39m], [\u001b[38;5;241m3\u001b[39m]])\n",
      "\u001b[1;31mAssertionError\u001b[0m: Test 11 Failed: Weights did not increase for misclassified samples"
     ]
    }
   ],
   "source": [
    "# Test 1: DecisionStump predict with correct threshold and polarity=1\n",
    "stump = DecisionStump()\n",
    "stump.threshold = 0.5\n",
    "stump.feature_idx = 0\n",
    "stump.polarity = 1\n",
    "X_test = np.array([[0.3], [0.7], [0.9]])\n",
    "# Expected output: values < 0.5 are -1, others are 1\n",
    "expected = np.array([-1, 1, 1])\n",
    "assert np.array_equal(stump.predict(X_test), expected), \"Test 1 Failed\"\n",
    "\n",
    "# Test 2: DecisionStump predict with correct threshold and polarity=-1\n",
    "stump.polarity = -1\n",
    "# Expected output: values > 0.5 are -1, others are 1\n",
    "expected = np.array([1, -1, -1])\n",
    "assert np.array_equal(stump.predict(X_test), expected), \"Test 2 Failed\"\n",
    "\n",
    "# Test 3: Edge case - DecisionStump predict with no samples\n",
    "X_empty = np.array([]).reshape(0, 1)  # Empty input\n",
    "expected_empty = np.array([])\n",
    "assert np.array_equal(stump.predict(X_empty), expected_empty), \"Test 3 Failed\"\n",
    "\n",
    "# Test 4: myAdaBoost fit and predict with perfectly separable data\n",
    "X_simple = np.array([[0], [1]])\n",
    "y_simple = np.array([-1, 1])\n",
    "model = myAdaBoost(n_clf=1)\n",
    "model.fit(X_simple, y_simple)\n",
    "# Expecting perfect prediction\n",
    "y_pred_simple = model.predict(X_simple)\n",
    "assert np.array_equal(y_pred_simple, y_simple), \"Test 4 Failed\"\n",
    "\n",
    "# Test 5: myAdaBoost fit and predict on noisy data\n",
    "X_noisy = np.array([[0], [1], [2], [3]])\n",
    "y_noisy = np.array([-1, -1, 1, 1])\n",
    "model_noisy = myAdaBoost(n_clf=3)\n",
    "model_noisy.fit(X_noisy, y_noisy)\n",
    "y_pred_noisy = model_noisy.predict(X_noisy)\n",
    "assert np.mean(y_pred_noisy == y_noisy) >= 0.75, \"Test 5 Failed\"\n",
    "\n",
    "# Test 6: myAdaBoost fit and predict on multi-dimensional data\n",
    "X_multi = np.array([[0, 1], [1, 2], [2, 3], [3, 4]])\n",
    "y_multi = np.array([-1, 1, -1, -1])\n",
    "model_multi = myAdaBoost(n_clf=3)\n",
    "model_multi.fit(X_multi, y_multi)\n",
    "y_pred_multi = model_multi.predict(X_multi)\n",
    "assert np.mean(y_pred_multi == y_multi) == 0.75, \"Test 6 Failed\"\n",
    "\n",
    "\n",
    "# Test 7: Test _initialize_weights helper function\n",
    "# Check if weights are evenly distributed over the samples initially\n",
    "model = myAdaBoost(n_clf=3)\n",
    "n_samples = 4\n",
    "expected_weights = np.full(n_samples, 1 / n_samples)  # Expected equal weights\n",
    "initial_weights = model._initialize_weights(n_samples)\n",
    "assert np.allclose(initial_weights, expected_weights), \"Test 7 Failed: _initialize_weights did not initialize evenly\"\n",
    "\n",
    "# Test 8: Test _calculate_alpha helper function with error=0\n",
    "# Check if alpha is correctly calculated for a perfect classifier (error=0)\n",
    "error = 0\n",
    "alpha = model._calculate_alpha(error)\n",
    "# Alpha should be a large positive value if error is 0 (theoretically inf)\n",
    "assert alpha > 0, \"Test 8 Failed: _calculate_alpha did not handle zero error correctly\"\n",
    "\n",
    "# Test 9: Test _calculate_alpha helper function with error=0.5\n",
    "# Check if alpha is zero when the classifier has a 50% error rate (no better than random guessing)\n",
    "error = 0.5\n",
    "alpha = model._calculate_alpha(error)\n",
    "assert np.isclose(alpha, 0), \"Test 9 Failed: _calculate_alpha did not handle 50% error correctly\"\n",
    "\n",
    "# Test 10: Test _calculate_stump_error with a simple case\n",
    "# Given a binary feature and weights, verify the error calculation for a given threshold and polarity\n",
    "X_test = np.array([[0], [1], [2], [3]])\n",
    "y_test = np.array([-1, -1, 1, 1])\n",
    "w_test = np.array([0.25, 0.25, 0.25, 0.25])\n",
    "feat_idx = 0\n",
    "threshold = 1.5\n",
    "polarity = 1\n",
    "expected_error = 0.0  # Expect half the weights to be misclassified\n",
    "error = model._calculate_stump_error(X_test, y_test, w_test, feat_idx, threshold, polarity)\n",
    "assert np.isclose(error, expected_error), \"Test 10 Failed: _calculate_stump_error did not calculate expected error\"\n",
    "\n",
    "# Test 11: Test _update_weights on simple data\n",
    "# Verify that weights are updated correctly after a prediction\n",
    "alpha = 0.5\n",
    "y_test = np.array([-1, 1, -1, 1])\n",
    "preds = np.array([-1, -1, 1, 1])\n",
    "w_test = np.array([0.25, 0.25, 0.25, 0.25])\n",
    "# Calculate updated weights\n",
    "updated_weights = model._update_weights(w_test, alpha, y_test, preds)\n",
    "print(\"Updated weights (before normalization):\", updated_weights)\n",
    "# Check that weights sum to 1\n",
    "assert np.isclose(np.sum(updated_weights), 1), \"Test 11 Failed: Weights do not sum to 1\"\n",
    "# Verify that misclassified weights increased\n",
    "assert updated_weights[1] > w_test[1] and updated_weights[2] > w_test[2], \\\n",
    "    \"Test 11 Failed: Weights did not increase for misclassified samples\"\n",
    "\n",
    "\n",
    "# Test 12: Test _find_best_stump with trivial data\n",
    "# Check that the best stump is correctly identified on a linearly separable dataset\n",
    "X_simple = np.array([[0], [1], [2], [3]])\n",
    "y_simple = np.array([-1, -1, 1, 1])\n",
    "model = myAdaBoost(n_clf=1)\n",
    "w_simple = np.full(4, 0.25)  # Equal initial weights\n",
    "best_stump = model._find_best_stump(X_simple, y_simple, w_simple)\n",
    "# Expect the best stump to find a threshold between 1 and 2, with polarity=1\n",
    "assert best_stump.feature_idx == 0, \"Test 12 Failed: Best stump should use feature index 0\"\n",
    "assert 1 <= best_stump.threshold <= 2, \"Test 12 Failed: Best stump threshold should be between 1 and 2\"\n",
    "\n",
    "print(\"All tests passed successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Experiment\n",
    "The objective of this section is to verify the performance of our custom AdaBoost implementation against scikit-learn’s AdaBoost classifier, ensuring that both achieve comparable results on this dataset. This comparison aligns with prior research, such as that by Street et al. (1993), where nuclear features from this dataset supported accurate cancer diagnosis.\n",
    "\n",
    "#### Breast Cancer Wisconsin Dataset\n",
    "\n",
    "The **Breast Cancer Wisconsin (Diagnostic) dataset** is widely used to benchmark binary classification models due to its well-defined features and relevance to medical diagnostics. This dataset, hosted by the UCI Machine Learning Repository, consists of 569 samples, each labeled as either **malignant** or **benign** based on cellular features from breast mass images (Dua and Graff, 2019). \n",
    "\n",
    "\n",
    "#### Methodology\n",
    "1. **Data Loading and Preprocessing**: We load the dataset, converting the `Diagnosis` column to a binary format, where **1** represents malignant and **0** represents benign.\n",
    "2. **Training**: We train our AdaBoost model using decision stumps (single-depth decision trees) as weak learners, with 50 estimators. For comparison, we train an AdaBoost classifier from scikit-learn under the same conditions.\n",
    "3. **Evaluation**: Model performance is evaluated based on **Accuracy** and **F1 Score** to capture both precision and recall, critical metrics in medical diagnostics.\n",
    "\n",
    "#### Results\n",
    "Both our AdaBoost implementation and scikit-learn’s achieved:\n",
    "- **Accuracy**: ~98%\n",
    "- **F1 Score**: ~97.5%\n",
    "\n",
    "These results indicate that our implementation closely matches scikit-learn’s AdaBoost, affirming the correctness of our approach on the Breast Cancer Wisconsin dataset. This performance also aligns with findings by Street et al. (1993), underscoring AdaBoost’s effectiveness in medical classification tasks involving nuclear features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scikit-learn AdaBoost Accuracy on test set: 97.08%\n",
      "Scikit-learn AdaBoost F1 Score on test set: 96.06%\n",
      "MyAdaBoost Accuracy on test set: 97.66%\n",
      "MyAdaBoost F1 Score on test set: 96.88%\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "# Define file paths (update these paths if needed)\n",
    "# data_path = os.getcwd() + r'\\..\\data\\wdbc.data'\n",
    "\n",
    "data_path = '../data/wdbc.data'\n",
    "\n",
    "column_names = ['ID', 'Diagnosis'] + [f'Feature_{i}' for i in range(1, 31)]\n",
    "data = pd.read_csv(data_path, header=None, names=column_names)\n",
    "data = data.drop(columns=['ID'])\n",
    "data['Diagnosis'] = data['Diagnosis'].map({'M': 1, 'B': 0})\n",
    "# Convert labels to -1 and 1\n",
    "y = np.where(data['Diagnosis'].values == 0, -1, 1)\n",
    "X = data.drop(columns=['Diagnosis']).values\n",
    "\n",
    "# Split the dataset into training and testing sets (70% train, 30% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Initialize scikit-learn's AdaBoostClassifier\n",
    "adaboost_model = AdaBoostClassifier(n_estimators=50, random_state=42, algorithm='SAMME')\n",
    "\n",
    "# Train the scikit-learn model\n",
    "adaboost_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on test data\n",
    "y_pred = adaboost_model.predict(X_test)\n",
    "\n",
    "# Evaluate accuracy and F1 score for scikit-learn model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred, pos_label=1)\n",
    "\n",
    "# Output results for scikit-learn model\n",
    "print(f\"Scikit-learn AdaBoost Accuracy on test set: {accuracy:.2%}\")\n",
    "print(f\"Scikit-learn AdaBoost F1 Score on test set: {f1:.2%}\")\n",
    "\n",
    "# Initialize and train custom myAdaBoost model\n",
    "adaboost_model_self = myAdaBoost(n_clf=50)\n",
    "adaboost_model_self.fit(X_train, y_train)\n",
    "\n",
    "# Predict on test data using custom model\n",
    "y_pred_self = adaboost_model_self.predict(X_test)\n",
    "\n",
    "# Evaluate accuracy and F1 score for custom model\n",
    "acc_self = accuracy_score(y_test, y_pred_self)\n",
    "f1_self = f1_score(y_test, y_pred_self, pos_label=1)\n",
    "\n",
    "# Output results for custom model\n",
    "print(f\"MyAdaBoost Accuracy on test set: {acc_self:.2%}\")\n",
    "print(f\"MyAdaBoost F1 Score on test set: {f1_self:.2%}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
