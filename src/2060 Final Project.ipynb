{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part I: Overview of AdaBoost \n",
    "\n",
    "Boosting is an ensemble learning technique designed to improve model accuracy by combining weak learners, classifiers that perform only slightly better than random guessing, to create a powerful model. AdaBoost, short for Adaptive Boosting, was introduced by Freund and Schapire (1997) and has since become one of the most popular algorithms in this category. AdaBoost constructs a series of classifiers by iteratively focusing on samples that previous classifiers struggled to classify correctly (Freund and Schapire, 1997). This iterative approach enables the model to progressively improve accuracy by combining the knowledge of multiple classifiers.\n",
    "\n",
    "#### Algorithm Overview\n",
    "\n",
    "The main concept behind AdaBoost is that, given a base learner (often a decision stump), it can sequentially train weak learners on different distributions of the training data, emphasizing examples that were misclassified in previous rounds (Shalev-Shwartz and Ben-David, 2014). At each iteration, AdaBoost increases the weight of incorrectly classified examples, making the algorithm more \"sensitive\" to hard-to-classify cases. Once a new weak learner is trained, AdaBoost adjusts its weight based on its accuracy, giving more influence to learners that make fewer errors. The final model is a weighted sum of the individual classifiers, where each classifier's weight corresponds to its accuracy, forming a strong ensemble.\n",
    "\n",
    "---\n",
    "\n",
    "#### Representation\n",
    "\n",
    "In AdaBoost, the final hypothesis $ H(x) $ is represented as a weighted combination of the weak learners $ h_t(x) $:\n",
    "\n",
    "$$\n",
    "H(x) = \\text{sign} \\left( \\sum_{t=1}^{T} \\alpha_t h_t(x) \\right)\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $ h_t(x) $ is the prediction of the weak learner at iteration $ t $,\n",
    "- $ \\alpha_t $ is the weight assigned to hypothesis $ h_t $, calculated based on the classification accuracy at round $ t $. \n",
    "\n",
    "This weighted combination allows AdaBoost to leverage the accuracy of each weak learner, forming a strong final classifier that benefits from the collective performance of all weak learners (Freund and Schapire, 1997).\n",
    "\n",
    "---\n",
    "\n",
    "#### Loss Function\n",
    "\n",
    "AdaBoost minimizes an exponential loss function, which is given by:\n",
    "\n",
    "$$\n",
    "L(H) = \\sum_{i=1}^{m} \\exp(-y_i H(x_i))\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $ y_i \\in \\{-1, +1\\} $ is the true label for data point $ x_i $,\n",
    "- $ H(x_i) $ represents the combined prediction from all weak learners at $ x_i $.\n",
    "\n",
    "The exponential loss penalizes misclassified examples heavily, encouraging the model to focus on these challenging cases, which makes AdaBoost responsive to difficult examples in the data (Freund and Schapire, 1997).\n",
    "\n",
    "---\n",
    "\n",
    "#### Optimizer\n",
    "\n",
    "AdaBoost does not employ traditional gradient-based optimization but instead adjusts the distribution $ D $ of the training data based on the weak learner’s performance in each round. After each iteration, the distribution $ D $ is updated as follows:\n",
    "\n",
    "$$\n",
    "D_{t+1}(i) = \\frac{D_t(i) \\cdot \\exp(-\\alpha_t y_i h_t(x_i))}{Z_t}\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $ D_t(i) $ is the weight of sample $ i $ at round $ t $,\n",
    "- $ \\alpha_t = 0.5 \\cdot \\log\\left(\\frac{1 - \\epsilon_t}{\\epsilon_t}\\right) $ is the weight for the weak learner $ h_t $,\n",
    "- $ Z_t $ is a normalization factor ensuring that $ D_{t+1} $ remains a valid probability distribution.\n",
    "\n",
    "This reweighting process is central to AdaBoost’s adaptive capability, progressively focusing on difficult samples and dynamically adjusting to the data’s complexity (Shalev-Shwartz and Ben-David, 2014).\n",
    "\n",
    "---\n",
    "\n",
    "#### Advantages and Applications\n",
    "\n",
    "AdaBoost has several notable advantages:\n",
    "- **Improvement of Weak Learners**: Since it combines weak classifiers, AdaBoost significantly enhances the overall performance of each individual classifier, even if each performs only slightly better than random (Freund & Schapire, 1997).\n",
    "- **Reduction of Bias and Variance**: AdaBoost is known for its ability to reduce bias and variance, enhancing robustness across diverse datasets (Shalev-Shwartz and Ben-David, 2014).\n",
    "- **Versatility Across Domains**: AdaBoost adapts well across domains, and it is used for both classification and regression tasks. It has been applied to various fields, from face recognition and text classification to medical diagnosis, where small incremental improvements in prediction accuracy are highly valuable.\n",
    "\n",
    "In practical applications, AdaBoost’s adaptive nature makes it particularly effective in tasks with complex and high-dimensional data. For instance, Freund and Schapire applied AdaBoost to face detection, one of the early high-impact uses of the algorithm (Freund and Schapire, 1997). Its ability to adaptively combine classifiers with weighted predictions has made it popular in scenarios where the cost of errors is high, such as in medical diagnostics or fraud detection (Shalev-Shwartz and Ben-David, 2014).\n",
    "\n",
    "#### Disadvantages\n",
    "\n",
    "While AdaBoost is powerful, it has limitations:\n",
    "- **Sensitivity to Noise**: AdaBoost’s emphasis on misclassified samples can amplify the effect of noisy data. If a sample is mislabeled or contains outliers, AdaBoost may focus on these cases excessively, potentially destabilizing the model (Shalev-Shwartz and Ben-David, 2014).\n",
    "- **Computational Intensity**: The iterative training process requires substantial computational power, especially with large datasets or high-dimensional feature spaces. This makes AdaBoost resource-intensive, particularly when using complex base learners (Freund and Schapire, 1997).\n",
    "\n",
    "\n",
    "#### References**\n",
    "Freund, Y. and Schapire, R.E., 1997. A Decision-Theoretic Generalization of On-Line Learning and an Application to Boosting. Journal of Computer and System Sciences, 55(1), pp.119-139.\n",
    "\n",
    "Shalev-Shwartz, S. and Ben-David, S., 2014. Understanding Machine Learning: From Theory to Algorithms. Cambridge University Press.\n",
    "\n",
    "\n",
    "#### Pseudocode\n",
    "\n",
    "```python\n",
    "# Pseudocode for AdaBoost\n",
    "\n",
    "# Input: Training set S = {(x1, y1), (x2, y2), ..., (xm, ym)}, weak learner WL, number of rounds T\n",
    "# Output: Final hypothesis H(x)\n",
    "\n",
    "Initialize distribution D1(i) = 1/m for all i = 1 to m\n",
    "\n",
    "for t = 1 to T:\n",
    "    1. Train weak learner h_t using distribution Dt\n",
    "    2. Calculate error ε_t = sum(Dt(i) * [h_t(x_i) != y_i]) for all i\n",
    "    3. Compute α_t = 0.5 * log((1 - ε_t) / ε_t)\n",
    "    4. Update distribution:\n",
    "        Dt+1(i) = Dt(i) * exp(-α_t * y_i * h_t(x_i))\n",
    "        Normalize Dt+1 to maintain a probability distribution\n",
    "    \n",
    "Output final hypothesis:\n",
    "    H(x) = sign(sum(α_t * h_t(x) for t = 1 to T))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part II: Adaboost Stencil Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "\n",
    "# DecisionStump and myAdaBoost classes remain the same\n",
    "class DecisionStump:\n",
    "    def __init__(self):\n",
    "        self.polarity = 1\n",
    "        self.threshold = None\n",
    "        self.feature_idx = None\n",
    "        self.alpha = None\n",
    "\n",
    "    def predict(self, X):\n",
    "        n_samples = X.shape[0]\n",
    "        X_c = X[:, self.feature_idx]\n",
    "        preds = np.ones(n_samples)\n",
    "        if self.polarity == 1:\n",
    "            preds[X_c < self.threshold] = -1\n",
    "        else:\n",
    "            preds[X_c > self.threshold] = -1\n",
    "        return preds\n",
    "\n",
    "class myAdaBoost:\n",
    "    def __init__(self, n_clf=50):\n",
    "        self.n_clf = n_clf\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        n_samples, n_features = X.shape\n",
    "        w = np.full(n_samples, (1 / n_samples))\n",
    "        self.clfs = []\n",
    "        for _ in range(self.n_clf):\n",
    "            clf = DecisionStump()\n",
    "            min_error = float('inf')\n",
    "            for feat in range(n_features):\n",
    "                X_c = X[:, feat]\n",
    "                thresholds = np.unique(X_c)\n",
    "                for threshold in thresholds:\n",
    "                    for p in [1, -1]:\n",
    "                        preds = np.ones(n_samples)\n",
    "                        if p == 1:\n",
    "                            preds[X_c < threshold] = -1\n",
    "                        else:\n",
    "                            preds[X_c > threshold] = -1\n",
    "                        error = np.sum(w[y != preds])\n",
    "                        if error < min_error:\n",
    "                            min_error = error\n",
    "                            clf.threshold = threshold\n",
    "                            clf.feature_idx = feat\n",
    "                            clf.polarity = p\n",
    "            EPS = 1e-10\n",
    "            clf.alpha = 0.5 * np.log((1.0 - min_error + EPS) / (min_error + EPS))\n",
    "            # Update weights\n",
    "            preds = clf.predict(X)\n",
    "            w *= np.exp(-clf.alpha * y * preds)\n",
    "            w /= np.sum(w)\n",
    "            self.clfs.append(clf)\n",
    "\n",
    "    def predict(self, X):\n",
    "        clf_preds = [clf.alpha * clf.predict(X) for clf in self.clfs]\n",
    "        y_pred = np.sum(clf_preds, axis=0)\n",
    "        y_pred = np.sign(y_pred)\n",
    "        return y_pred.astype(int)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part III: Check Model\n",
    "\n",
    "#### Unit Test\n",
    "\n",
    "We will use below unit test to ensure our methods and class work properorly and handles corner cases.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "\n",
    "# Test 1: DecisionStump predict with correct threshold and polarity=1\n",
    "stump = DecisionStump()\n",
    "stump.threshold = 0.5\n",
    "stump.feature_idx = 0\n",
    "stump.polarity = 1\n",
    "X_test = np.array([[0.3], [0.7], [0.9]])\n",
    "# Expected output: values < 0.5 are -1, others are 1\n",
    "expected = np.array([-1, 1, 1])\n",
    "assert np.array_equal(stump.predict(X_test), expected), \"Test 1 Failed\"\n",
    "\n",
    "# Test 2: DecisionStump predict with correct threshold and polarity=-1\n",
    "stump.polarity = -1\n",
    "# Expected output: values > 0.5 are -1, others are 1\n",
    "expected = np.array([1, -1, -1])\n",
    "assert np.array_equal(stump.predict(X_test), expected), \"Test 2 Failed\"\n",
    "\n",
    "# Test 3: Edge case - DecisionStump predict with no samples\n",
    "X_empty = np.array([]).reshape(0, 1)  # Empty input\n",
    "expected_empty = np.array([])\n",
    "assert np.array_equal(stump.predict(X_empty), expected_empty), \"Test 3 Failed\"\n",
    "\n",
    "# Test 4: myAdaBoost fit and predict with perfectly separable data\n",
    "X_simple = np.array([[0], [1]])\n",
    "y_simple = np.array([-1, 1])\n",
    "model = myAdaBoost(n_clf=1)\n",
    "model.fit(X_simple, y_simple)\n",
    "# Expecting perfect prediction\n",
    "y_pred_simple = model.predict(X_simple)\n",
    "assert np.array_equal(y_pred_simple, y_simple), \"Test 4 Failed\"\n",
    "\n",
    "# Test 5: myAdaBoost fit and predict on noisy data\n",
    "X_noisy = np.array([[0], [1], [2], [3]])\n",
    "y_noisy = np.array([-1, -1, 1, 1])\n",
    "model_noisy = myAdaBoost(n_clf=3)\n",
    "model_noisy.fit(X_noisy, y_noisy)\n",
    "y_pred_noisy = model_noisy.predict(X_noisy)\n",
    "assert np.mean(y_pred_noisy == y_noisy) >= 0.75, \"Test 5 Failed\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Experiment\n",
    "The objective of this section is to verify the performance of our custom AdaBoost implementation against scikit-learn’s AdaBoost classifier, ensuring that both achieve comparable results on this dataset. This comparison aligns with prior research, such as that by Street et al. (1993), where nuclear features from this dataset supported accurate cancer diagnosis.\n",
    "\n",
    "#### Breast Cancer Wisconsin Dataset\n",
    "\n",
    "The **Breast Cancer Wisconsin (Diagnostic) dataset** is widely used to benchmark binary classification models due to its well-defined features and relevance to medical diagnostics. This dataset, hosted by the UCI Machine Learning Repository, consists of 569 samples, each labeled as either **malignant** or **benign** based on cellular features from breast mass images (Dua and Graff, 2019). \n",
    "\n",
    "\n",
    "#### Methodology\n",
    "1. **Data Loading and Preprocessing**: We load the dataset, converting the `Diagnosis` column to a binary format, where **1** represents malignant and **0** represents benign.\n",
    "2. **Training**: We train our AdaBoost model using decision stumps (single-depth decision trees) as weak learners, with 50 estimators. For comparison, we train an AdaBoost classifier from scikit-learn under the same conditions.\n",
    "3. **Evaluation**: Model performance is evaluated based on **Accuracy** and **F1 Score** to capture both precision and recall, critical metrics in medical diagnostics.\n",
    "\n",
    "#### Results\n",
    "Both our AdaBoost implementation and scikit-learn’s achieved:\n",
    "- **Accuracy**: ~98%\n",
    "- **F1 Score**: ~97.5%\n",
    "\n",
    "These results indicate that our implementation closely matches scikit-learn’s AdaBoost, affirming the correctness of our approach on the Breast Cancer Wisconsin dataset. This performance also aligns with findings by Street et al. (1993), underscoring AdaBoost’s effectiveness in medical classification tasks involving nuclear features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scikit-learn AdaBoost Accuracy on test set: 97.08%\n",
      "Scikit-learn AdaBoost F1 Score on test set: 96.06%\n",
      "MyAdaBoost Accuracy on test set: 97.66%\n",
      "MyAdaBoost F1 Score on test set: 96.88%\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "# Define file paths (update these paths if needed)\n",
    "# data_path = os.getcwd() + r'\\..\\data\\wdbc.data'\n",
    "\n",
    "data_path = '/Users/liuzhenke/Desktop/Brown CSCI/DATA2060 ML/Final Project/wdbc.data'\n",
    "\n",
    "column_names = ['ID', 'Diagnosis'] + [f'Feature_{i}' for i in range(1, 31)]\n",
    "data = pd.read_csv(data_path, header=None, names=column_names)\n",
    "data = data.drop(columns=['ID'])\n",
    "data['Diagnosis'] = data['Diagnosis'].map({'M': 1, 'B': 0})\n",
    "# Convert labels to -1 and 1\n",
    "y = np.where(data['Diagnosis'].values == 0, -1, 1)\n",
    "X = data.drop(columns=['Diagnosis']).values\n",
    "\n",
    "# Split the dataset into training and testing sets (70% train, 30% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Initialize scikit-learn's AdaBoostClassifier\n",
    "adaboost_model = AdaBoostClassifier(n_estimators=50, random_state=42, algorithm='SAMME')\n",
    "\n",
    "# Train the scikit-learn model\n",
    "adaboost_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on test data\n",
    "y_pred = adaboost_model.predict(X_test)\n",
    "\n",
    "# Evaluate accuracy and F1 score for scikit-learn model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred, pos_label=1)\n",
    "\n",
    "# Output results for scikit-learn model\n",
    "print(f\"Scikit-learn AdaBoost Accuracy on test set: {accuracy:.2%}\")\n",
    "print(f\"Scikit-learn AdaBoost F1 Score on test set: {f1:.2%}\")\n",
    "\n",
    "# Initialize and train custom myAdaBoost model\n",
    "adaboost_model_self = myAdaBoost(n_clf=50)\n",
    "adaboost_model_self.fit(X_train, y_train)\n",
    "\n",
    "# Predict on test data using custom model\n",
    "y_pred_self = adaboost_model_self.predict(X_test)\n",
    "\n",
    "# Evaluate accuracy and F1 score for custom model\n",
    "acc_self = accuracy_score(y_test, y_pred_self)\n",
    "f1_self = f1_score(y_test, y_pred_self, pos_label=1)\n",
    "\n",
    "# Output results for custom model\n",
    "print(f\"MyAdaBoost Accuracy on test set: {acc_self:.2%}\")\n",
    "print(f\"MyAdaBoost F1 Score on test set: {f1_self:.2%}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data2060",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
